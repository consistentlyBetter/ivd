---
output: github_document
bibliography: inst/ref.bib
nocite: '@*'
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
---

<!-- README.md is generated from README.Rmd. Please edit that file -->
<!-- knit with rmarkdown::render("README.Rmd", output_format = "md_document") -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  cache = FALSE
)
```

# Individual Variance Detection


<!-- badges: start -->
[![R-CMD-check](https://github.com/ph-rast/ivd/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/ph-rast/ivd/actions/workflows/R-CMD-check.yaml)
[![codecov](https://codecov.io/gh/consistentlyBetter/ivd/graph/badge.svg?token=SD0PM5BVIL)](https://codecov.io/gh/consistentlyBetter/ivd)
<!-- badges: end -->

*ivd* is an R package for random effects selection in the scale part of Mixed Effects Location Scale Modlels (MELSM). `ivd()` fits a random intercepts model with a spike-and-slab prior on the random effects of the scale.

## Installation

This package can be installed with 

``` r
# install.packages("devtools")
devtools::install_github("consistentlybetter/ivd")
```

## Example


```{r example}
library(ivd)
library(data.table)
```
## Data
The illustration uses openly accessible data from The Basic Education Evaluation System (Saeb) conducted by Brazil's National Institute for Educational Studies and Research (Inep), available at <https://www.gov.br/inep/pt-br/areas-de-atuacao/avaliacao-e-exames-educacionais/saeb/resultados>. It is also available as the `saeb` dataset in the `ivd` package.

Separate within- from between-school effects. That is, besides `student_ses`, compute `school_ses`. 

```{r}
# Calculate school-level SES
school_ses <- saeb[, .(school_ses = mean(student_ses, na.rm = TRUE)), by = school_id]

# Join the school_ses back to the original dataset
saeb <- saeb[school_ses, on = "school_id"]

# Grand mean center school ses
saeb$school_ses <- c(scale(saeb$school_ses, scale = FALSE))

head(saeb )
```

Illustration of school level variability:
```{r}
plot0 <- ggplot( data = saeb, aes( x = school_id, y = math_proficiency) )
plot0 + geom_point(aes(color =  school_id), show.legend =  FALSE)
```



## Estimate Model

We will predict `math_proficiency` which is a standardized variable capturing math proficiency at the end of grade 12.

Both, location (means) and scale (residual variances) are modeled as a function of student and school SES. Note that the formula objects for both location and scale follow `lme4` notation.

```{r warning=FALSE}
out <- ivd(location_formula = math_proficiency ~ student_ses * school_ses + (1|school_id),
           scale_formula =  ~ student_ses * school_ses + (1|school_id),
           data = saeb,
           niter = 5000, nburnin = 5000, WAIC = TRUE, workers = 4)
```

The summary shows the fixed and random effects and it returns all posterior inclusion probabilities (PIP) for each one of the 160 schools' residual variance random effects. The PIP returns the probability of a school belonging to the slab, that is, the probability of the model having to include the random scale effect. 

In other words, large PIP's indicate schools that are substantially deviating from the fixed scale effects either because they are much _more_ or much _less_  variable compared to other schools in math proficiency.

One can readily convert those PIP's to odds, indicating that a school with a PIP = .75 is three times as likely to belonging to the slab than belonging to the spike. With an .50 inclusion prior, these odds can be readily interpreted as Bayes Factors.

```{r}
s_out <- summary(out)
```

## Plots

### Posterior inclusion probability plot (PIP)

```{r}
plot(out, type = "pip" )
```
### PIP vs. Within-cluster SD

```{r}
plot(out, type =  "funnel")
```

### PIP vs. math achievement

```{r}
plot(out, type =  "outcome")
```


### Diagnostic plots based on coda plots:

```{r}
codaplot(out, parameter =  "beta[1]")
codaplot(out, parameter =  "R[2, 1]")
```

# References
